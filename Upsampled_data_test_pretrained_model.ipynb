{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenBCI EEG data analysis (reading vs writing state)\n",
    "Based on implementation by Viacheslav Nesterov (https://github.com/Vyachez/Project_BCI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from numpy import argmax\n",
    "import seaborn as sns\n",
    "\n",
    "from imp import reload\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable and function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINITIONS\n",
    "path_r = 'openbci-data/OpenBCI-RAW-2019-02-19_23-19-58-read.txt'\n",
    "path_w = 'openbci-data/OpenBCI-RAW-2019-02-19_23-23-02-write.txt'\n",
    "dur_model = 990\n",
    "secbatch = 10  \n",
    "margin = 15000                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_timeseries(dset):\n",
    "    dset = dset.drop(columns=[0,9,10,11,12])\n",
    "    dset = dset.reset_index(drop=True)\n",
    "    dset.index=pd.to_datetime(dset[13],unit='ms')\n",
    "    return dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(dset, methodname):\n",
    "    dset.index = pd.to_datetime(dset[13],unit='ms')\n",
    "    upsampled = dset.resample('900000ns').mean().interpolate(method=methodname)\n",
    "    return upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_upsampled(dset):\n",
    "    dset['time'] = dset[13]\n",
    "    dset['sec']  = dset.index\n",
    "    dset['sec']  = dset['sec'].astype(str).str[11:-7]\n",
    "    dset = dset.drop(columns=[13])\n",
    "    dset = dset.reset_index(drop=True)\n",
    "    dset = dset.drop(columns=['time'])\n",
    "    dset = dset.reset_index(drop=True)\n",
    "    return dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_check(dset, dur):\n",
    "    for i in np.unique(dset['sec']):\n",
    "        if len(dset['sec'].loc[dset['sec'] == i]) != dur:\n",
    "            print('Seconds are not equal!')\n",
    "            print(np.array([i, len(dset['sec'].loc[dset['sec'] == i])]))\n",
    "    print(\"Check completed for balanced intervals!\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balancing intervals to set length (optimal number of rows)\n",
    "def balance_intervals(dset, int_no):\n",
    "    \"\"\" dset - dataset to cleanup\n",
    "        int_no - min length of intervals within one second\"\"\"\n",
    "    idarr = np.array([[i, len(dset['sec'].loc[dset['sec'] == i])] for i in np.unique(dset['sec'])])\n",
    "    idarr[0:3]\n",
    "    for i in idarr:\n",
    "        if int(i[1]) < int_no:\n",
    "            date = i[0]\n",
    "            # removing short/incomplete\n",
    "            dset = dset.drop(index=dset.loc[dset['sec'] == date].index)\n",
    "        elif int(i[1]) > int_no:\n",
    "            date = i[0]\n",
    "            end_ind = dset.loc[dset['sec'] == date].index[-1]\n",
    "            cut_ind = dset.loc[dset['sec'] == date].index[int_no-1]\n",
    "            # cutting excessive\n",
    "            dset = dset.drop(dset.index[cut_ind:end_ind])\n",
    "        dset = dset.reset_index(drop=True)\n",
    "    return dset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if all of intervals have the same length by second\n",
    "def check_sec_int(dset):\n",
    "    return np.array([[i, len(dset['sec'].loc[dset['sec'] == i])] for i in np.unique(dset['sec'])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance_clean(dset, var):\n",
    "    \"\"\" dset - dataset to cleanup\n",
    "        var - max variance - all that above is to catch and remove\n",
    "    \"\"\"\n",
    "    for chan in range(8):\n",
    "        for sec in np.unique(dset['sec']):\n",
    "            min_edge = min(dset.loc[dset['sec'] == sec][chan+1])\n",
    "            max_edge = max(dset.loc[dset['sec'] == sec][chan+1])\n",
    "            variance = max_edge - min_edge\n",
    "            idx = dset.loc[dset[chan+1] == max_edge].index[0]\n",
    "            #print(variance)\n",
    "            if variance > var:\n",
    "                print('Channel {} | Second {} | Index {} | Variance:] = {}'.format(chan+1, sec, idx, variance))\n",
    "                dset = dset.drop(index=dset.loc[dset['sec'] == sec].index)\n",
    "                # reseting the index\n",
    "                dset = dset.reset_index(drop=True)\n",
    "                print('Dropped')\n",
    "    print('Cleaned spikes larger than', var)\n",
    "    return dset   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seconds(dset):\n",
    "    '''takes dataset as a  argument and utputs\n",
    "    number of unique seconds'''\n",
    "    print(\"\\nSeconds in dataset now: \", len(np.unique(dset['sec'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling function\n",
    "def scaler(dset, secs, dur):\n",
    "    '''Scaling function takes dataset of 8 channels\n",
    "    and returns rescaled dataset.\n",
    "    Rescales by interval (second).\n",
    "    arg 'secs' is number of seconds to take into one scaling tensor.\n",
    "    Scaling is between 1 and 0\n",
    "    All datasets for training should be equalized'''\n",
    "    # first - getting length of dataset modulo number of seconds for scaling\n",
    "    intlv = secs*dur\n",
    "    if len(dset['sec'])/intlv > 1:\n",
    "        lendset = int(len(dset['sec'])/intlv)\n",
    "        dset = dset[0:lendset*intlv]\n",
    "        dset = dset.reset_index(drop=True)\n",
    "    else:\n",
    "        print(\"Inappropriate length of dataset. Too short. Set up less seconds for batch or choose another dataset.\")\n",
    "    seconds(dset)\n",
    "    # now scaling\n",
    "    if balance_check(dset, dur=dur):\n",
    "        for chan in range(8):\n",
    "            for i in range(int(len(dset['sec'])/intlv)):\n",
    "                tmpdat = dset.loc[i*intlv:i*intlv+intlv-1, chan+1]\n",
    "                tmpdat = (tmpdat-min(tmpdat))/(max(tmpdat)-min(tmpdat))  \n",
    "                dset.loc[i*intlv:i*intlv+intlv-1, chan+1]= tmpdat\n",
    "        dset = dset.reset_index(drop=True)\n",
    "        print(\"Dataset has been rescaled \\n\")\n",
    "        return dset\n",
    "    else:\n",
    "        print(\"\\nDataset intervals are not balanced! Check the code and functions order.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing function\n",
    "def preprocess(fl, dur=dur_model, var=margin, secbatch=secbatch):\n",
    "    '''\"fl\" (fullpath) - Takes dataset file\n",
    "    \"dur\" - length of second - number of items per second\n",
    "    \"var\" - variance of wave that need to be trimmed\n",
    "    \"secbatch\" - batch of seconds to scale\n",
    "    Returns balanced preprocessed data'''\n",
    "    # loading the data\n",
    "    dset = pd.read_csv(fl, sep=\",\", header=None)\n",
    "    # convert dataframe into time series in order to upsample\n",
    "    dset = prepare_timeseries(dset)\n",
    "    # upsample the data\n",
    "    dset = upsample(dset,'linear')\n",
    "    # clean upsampled data\n",
    "    dset = clean_upsampled(dset)\n",
    "    # checking length\n",
    "    seconds(dset)\n",
    "    # cleaning spikes\n",
    "    dset = variance_clean(dset, var)\n",
    "    # checking length again\n",
    "    seconds(dset)\n",
    "    # balancing seconds intervals to have same length\n",
    "    dset = balance_intervals(dset, dur)\n",
    "    balance_check(dset, dur)\n",
    "    # trimming and scaling to fit the integrity for data prep\n",
    "    dset = scaler(dset, secbatch, dur)\n",
    "    return dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full 8 channels conversion\n",
    "# each second to be converted in one n-dimentional array\n",
    "def conversion_8(df):\n",
    "    dat_lst = []\n",
    "    for s in np.unique(df['sec']):\n",
    "        dat_lst.append(np.array(df[[1,2,3,4,5,6,7,8]].loc[df['sec'] == s]))\n",
    "    return np.array(dat_lst, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# model architecture\n",
    "num_classes = 2\n",
    "model = Sequential()\n",
    "model.add(Conv1D(32, kernel_size=(3), padding='same',\n",
    "                 activation='relu',\n",
    "                 input_shape=(990, 8,)))\n",
    "model.add(BatchNormalization(axis=1))\n",
    "model.add(MaxPooling1D(pool_size=5, padding='same'))\n",
    "model.add(Conv1D(64, (3), activation='relu'))\n",
    "model.add(BatchNormalization(axis=1))\n",
    "model.add(MaxPooling1D(pool_size=3, padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 990, 32)           800       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 990, 32)           3960      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 198, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 196, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 196, 64)           784       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 66, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4224)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                135200    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 147,018\n",
      "Trainable params: 144,646\n",
      "Non-trainable params: 2,372\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model using a loss function and an optimizer.\n",
    "adam = optimizers.Adam(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Model with the Best Classification Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model wights taken from (https://github.com/Vyachez/Project_BCI)\n",
    "model.load_weights('r_w.weights.best.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Seconds in dataset now:  36\n",
      "Cleaned spikes larger than 15000\n",
      "\n",
      "Seconds in dataset now:  36\n",
      "Check completed for balanced intervals!\n",
      "\n",
      "Seconds in dataset now:  30\n",
      "Check completed for balanced intervals!\n",
      "Dataset has been rescaled \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "read_test = preprocess(path_r, dur=dur_model, var=margin, secbatch=secbatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Seconds in dataset now:  40\n",
      "Cleaned spikes larger than 15000\n",
      "\n",
      "Seconds in dataset now:  40\n",
      "Check completed for balanced intervals!\n",
      "\n",
      "Seconds in dataset now:  30\n",
      "Check completed for balanced intervals!\n",
      "Dataset has been rescaled \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "write_test = preprocess(path_w, dur=dur_model, var=margin, secbatch=secbatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_dat_xt = conversion_8(read_test)\n",
    "w_dat_xt = conversion_8(write_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict any class\n",
    "def predict_all(data, st):\n",
    "    '''takes data - array with tensors for prediction\n",
    "    predict any state depending on labels match '''\n",
    "    timer = []\n",
    "    sec = 0\n",
    "    count_state = []\n",
    "    for i in data:\n",
    "        sec += 1\n",
    "        count_all = timer.append(sec)\n",
    "        if model.predict(np.array([i]))[0][0] > 0.5:\n",
    "            count_state.append(0)\n",
    "            #print('Second {}: {}'.format(sec, states[0]))\n",
    "        elif model.predict(np.array([i]))[0][1] > 0.5:\n",
    "            count_state.append(1)\n",
    "            #print('Second {}: {}'.format(sec, states[1]))\n",
    "        else:\n",
    "            count_state.append(2)\n",
    "            #print('Second {}: {}'.format('Unknown'))\n",
    "                  \n",
    "    read_count = np.count_nonzero(np.array(count_state) == 0)\n",
    "    write_count = np.count_nonzero(np.array(count_state) == 1)\n",
    "    unknown_count = np.count_nonzero(np.array(count_state) == 2)\n",
    "\n",
    "    if st == 'r':\n",
    "        print('\\nPredicted reading: {} sec | unknown: {} sec | from total {} sec of READING set'.format(read_count,\n",
    "                                                                                                     unknown_count,\n",
    "                                                                                             len(count_state)))\n",
    "        print('Prediction accuracy for READING on new environment dataset is: {}%'.format(int(read_count/len(count_state)*100)))\n",
    "        st_int_r = 1\n",
    "        st_int_w = 0\n",
    "    elif st == 'w':\n",
    "        print('\\nPredicted writing: {} sec | unknown: {} sec | from total {} sec of WRITING set'.format(write_count,\n",
    "                                                                                                     unknown_count,\n",
    "                                                                                             len(count_state)))\n",
    "        print('Prediction accuracy for WRITING on new environment dataset is: {}%'.format(int(write_count/len(count_state)*100)))\n",
    "        st_int_r = 0\n",
    "        st_int_w = 1\n",
    "    return {st_int_r: int(read_count/len(count_state)*100), st_int_w: int(write_count/len(count_state)*100)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted reading: 26 sec | unknown: 0 sec | from total 30 sec of READING set\n",
      "Prediction accuracy for READING on new environment dataset is: 86%\n",
      "\n",
      "Predicted writing: 3 sec | unknown: 0 sec | from total 30 sec of WRITING set\n",
      "Prediction accuracy for WRITING on new environment dataset is: 10%\n",
      "\n",
      "Real model accuracy is: 48.0%\n"
     ]
    }
   ],
   "source": [
    "benchmark = [predict_all(r_dat_xt, 'r'), predict_all(w_dat_xt, 'w')]\n",
    "acc = []\n",
    "for b in benchmark:\n",
    "    acc.append(b[1])\n",
    "print('\\nReal model accuracy is: {}%'.format(np.average([acc])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
